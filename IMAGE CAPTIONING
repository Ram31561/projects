import torch
import torch.nn as nn
import torchvision.models as models
from torchvision import transforms
from PIL import Image

# Load pre-trained ResNet model (remove the classification layer)
resnet = models.resnet50(pretrained=True)
modules = list(resnet.children())[:-1]
resnet = nn.Sequential(*modules)

# Freeze the ResNet layers
for param in resnet.parameters():
    param.requires_grad = False

# Define the RNN-based captioning model
class CaptioningModel(nn.Module):
    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):
        super(CaptioningModel, self).__init__()
        self.embed = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.linear = nn.Linear(hidden_size, vocab_size)

    def forward(self, features, captions):
        embeddings = self.embed(captions)
        inputs = torch.cat((features.unsqueeze(1), embeddings), dim=1)
        outputs, _ = self.lstm(inputs)
        out = self.linear(outputs)
        return out

# Define image preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])
def generate_caption(image_path, max_length=20):
    # Load and preprocess the image
    image = Image.open(image_path)
    image = transform(image).unsqueeze(0)

    # Extract image features using ResNet
    with torch.no_grad():
        features = resnet(image)
    features = features.view(1, -1)

    # Generate captions using the trained model
    caption = ["<start>"]
    for _ in range(max_length):
        input_caption = [vocab.word2idx[word] for word in caption]
        input_caption = torch.tensor(input_caption).unsqueeze(0).to(device)
        outputs = model(features.to(device), input_caption)
        predicted = outputs.argmax(2)[:, -1].item()
        caption.append(vocab.idx2word[predicted])
        if caption[-1] == "<end>":
            break

    return " ".join(caption[1:-1])

# Example usage
generated_caption = generate_caption("image.jpg")
print("Generated Caption:", generated_caption)
